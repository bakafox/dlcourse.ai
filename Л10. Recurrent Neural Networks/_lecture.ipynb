{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Применение рекуррентных нейронных сетей (recurrent neural networks) в задачах распознавания естественного языка. Детали архитектуры LSTM.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# В предыдущей серии...\n",
    "\n",
    "![text](attach1.png) ![text](attach2.png) ![text](attach3.png)\n",
    "\n",
    "*(если не понятно, что здесь происходит, стоит вернуться к прошлой лекции!)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Универсальные и гибкие, часто используются в NLP, тренируемые... конечно же, это\n",
    "\n",
    "# Рекуррентные нейронные сети\n",
    "\n",
    "*По мотивам почта The Unreasonable Effectiveness of Recurrent Neural Networks*\n",
    "\n",
    "Как говорилось в прошлой лекции, в самом NLP очень много разнообразных юзкейсов — они описаны на картинке ниже (красный — входные слова, синий — выходные слова):\n",
    "\n",
    "![alt text](attach4.png)\n",
    "\n",
    "- One to One (ближе к классической нейронной сети — вход и выход фиксированного размера)\n",
    "- One to Many (с этим и другими видами ниже мы и познакомимся)\n",
    "- Many to One (например, анализ характера предложения)\n",
    "- Many to Many (например, перевод текстов)\n",
    "\n",
    "Но как должна выглядеть сеть, которая может работать с переменным количеством данных на вход и выход?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Архитектура\n",
    "\n",
    "Возьмём какой-то один линейный слой:\n",
    "\n",
    "![alt text](attach5.png)\n",
    "\n",
    "И начнём **выход** этого слоя давать на **вход**, помимо входных данных, следующему слою:\n",
    "\n",
    "![alt text](attach6.png)\n",
    "\n",
    "То есть у такой сети есть возможность передать себе в \"будущее\" некоторые состояние — и мы тренируем такую систему, где сеть выучивается, как некоторым образом закодировать и передать на следующем шаге, чтобы \"знать\" результаты прошлых шагов — и иметь доступ ко всей цепочке последовательности.\n",
    "\n",
    "Получается такая своего рода рекурсия — поэтому сеть и называется **Recurrent Neural Network (рекурсивная нейроннасть сеть)**. Реализация в коде выглядит так:\n",
    "\n",
    "![alt text](attach7.png)\n",
    "\n",
    "Другое представление этого же принципа работы, из блогпоста А. Карпатого:\n",
    "\n",
    "![alt text](attach8.png)\n",
    "\n",
    "![alt text](attach9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример: генерация текста\n",
    "\n",
    "### Training\n",
    "\n",
    "Разобьём весь наш текст на последовательность символов. Также добавим специальные символы, означаютщие, например, о начале и конце строки:\n",
    "\n",
    "```\n",
    "Hello\n",
    "--->\n",
    "'[НАЧАЛО_СТРОКИ]' 'H' 'e' 'l' 'l' 'o'\n",
    "--->\n",
    "'H' 'e' 'l' 'l' 'o' '[КОНЕЦ_СТРОКИ]'\n",
    "```\n",
    "\n",
    "Как выглядит наш словарь для примера выше:\n",
    "\n",
    "![alt text](attach12.png)\n",
    "\n",
    "То есть сеть тренируется по прошлым символам (являются контекстом) предсказывать следующий символ. Ей нужно обучиться, какие символы чаще всего следуют за другими.\n",
    "\n",
    "### Inference\n",
    "\n",
    "Например, подадим на вход спецсимвол начала сткрои и посмотрим, что будет дальше:\n",
    "\n",
    "```\n",
    "'[НАЧАЛО_СТРОКИ]' -> 'H'\n",
    "--->\n",
    "'H' -> 'e'\n",
    "--->\n",
    "'e' -> ...\n",
    "--->\n",
    "... -> '[КОНЕЦ_СТРОКИ]'\n",
    "```\n",
    "\n",
    "... и, если делать train достаточно долго, даже такая несложная, генерирующая посимвольно, простая модель даёт относительно связный текст! Из минусов — в отличии от word2vec, с такой моделью Transfer Learning работает крайне плохо.\n",
    "\n",
    "![alt text](attach11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM), 1997\n",
    "\n",
    "Но есть небольшой секрет в этом прекрасном мире... Если написать код, как в примере выше, сеть работать НЕ БУДЕТ:\n",
    "\n",
    "![alt text](attach7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Но почему?\n",
    "\n",
    "![alt text](attach13.png)\n",
    "\n",
    "![alt text](attach14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И что же делать?\n",
    "\n",
    "Делать шаг более по-умному, нежели просто делать умножение на матрицу. Наиболее популярный вариант такой реализации называется **Long Short-Term Memory**.\n",
    "\n",
    "Архитектура, которую мы разбирали до этого, называется **Vanilla RNN**:\n",
    "\n",
    "![alt text](attach8.png)\n",
    "\n",
    "А вот так страшненько выглядит наш спаситель:\n",
    "\n",
    "![alt text](attach16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как ОНО работает?\n",
    "\n",
    "Самое важное изменение — мы передаём не один вектор, а два — помимо hidden layer ($h$), мы будем передавать self-state ($C$). \n",
    "\n",
    "И если в вычислении первого ничего принципиально не меняется (т.е. он всё так же будет подвержен тем же проблемам), то в последнем мы будем делать изменения очень осторожно, чтобы градиенты по нему всегда протекали хорошо из прошлого шага в следующий.\n",
    "\n",
    "![alt text](attach15.png)\n",
    "\n",
    "Помимо этого, мы введём gate-ы ($\\sigma = \\frac{1}{1+e^{-x}}$) — различные вектора коэффициентов, на которые мы будем перемножать некоторые входные значения:\n",
    "\n",
    "- Первым таким gate-ом будет **Forget gate**. Он выдаёт вектор от 0 до 1 для каждой из компонент, который говорит для каждой из них, сколько из неё стоит забыть;\n",
    "- Вторым таким gate-ом будем **Input gate**. Он выдаёт вектор от 0 до 1 для каждой из компонент, на сколько стоит домножить добавку этого шага к $c$;\n",
    "- Затем происходит **Cell Update**. На выход self-state шага пойдёт $C_t$ (self-state после всех манипуляциями).\n",
    "- Наконец, идёт **Output Gate**. Он говорит, каким образом мы получаем $h_t$ (hidden layer после всех манипуляций, он пойдёт на выход)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Почему ОНО работает?\n",
    "\n",
    "![alt text](attach17.png)\n",
    "\n",
    "Для $C$ — взяли с прошлого, умножили на forget gate, добаивли добавку и пустили дальше. Получился такой типа хайвэй, на котором градиент (справа сверху обозначен стрелкою) протекает намного лучше и меньше затухает."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Варианты LSTM\n",
    "\n",
    "На текущий\\* момент, существует много разных модификаций LSTM, например с Peephole Connections или No Input Gate.\n",
    "\n",
    "Но как правило, используют либо обычный LSTM, либо GRU (версия LSTM от Google с упрощёнными вычислениями):\n",
    "\n",
    "![alt text](attach18.png)\n",
    "\n",
    "Также, до изобретения LSTM, люди часто использовали Bidirectional RNN — рекурсивная нейросеть и отдельными словами для forward и backward проходом. Но, для задач типа генерации текста она не подходит. В современном мире такой подод исопльзуется в модификации Bidirectional LSTM для случаев, когда нужен очень очень хорошо протексающий по огромному числу слоёв и не затухающий в процессе градиент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM (иногда + word2vec) — очень эффективная комбинация для решения задач NLP!\n",
    "\n",
    "![alt text](attach19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Более \"низкоуровненый\" разбор работы на примере Part of Speech Tagging:\n",
    "\n",
    "![alt text](attach20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*В следующий раз будет про ЕЩЁ более крутые инструменты для решения проблем, близкие к SoTA на момент 2019.*\n",
    "\n",
    "***А пока — пока!***"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
