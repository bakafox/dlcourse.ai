{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Краткий обзор области обработки естественного языка и применения deep learning к ней на примере word2vec.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Что такое NLP?\n",
    "\n",
    "*Нет, не нейролингвистическое программирование (или как там эта хрень из 90-х называлась).*\n",
    "\n",
    "Определение из Педивикии: **Обработка естественного языка (Natural Language Processing, NLP)** — общее направление ИИ и математической лингвистики, изучающее проблемы комьпютерного анализа и синтеза естественных языков.\n",
    "\n",
    "Переводя на русский: есть некоторый текст на естественном языке, который написал человек, и мы хотим научить компьютер \"понимать\", что там написано, и/или \"выражать идеи\" на этом языке.\n",
    "\n",
    "Пример использования: голосовые ассистенты (Google Now, Siri, ~~Cortana~~), чатботы (особенно теперь)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Pipeline (обобщённый алгоритм работы)\n",
    "\n",
    "![alt text](attach1.png)\n",
    "\n",
    "1. **Вход** — загружаем текст (или другие данные естественного языка, переведённые в текст) как входные данные;\n",
    "\n",
    "2. **Морфология** — рзаобраться, какая смысловая нагрузка для слов и из каких частей они состоят;\n",
    "\n",
    "3. **Синтаксис** — (этот шаг можно пропустить) как-то распарсить текст, например, выделить только нужный текст с веб-сайта;\n",
    "\n",
    "4. **Семантика** — понять/классифицировать текст (например, положительный или отрицательный ли отзыв);\n",
    "\n",
    "5. **Контекст** — проанализировать, что происходило до этого текста и как он с ними \"вяжется\" (например, в случае чат-ботов, это предыдущие команды пользователя и ответы ему)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как это делают без нейросетей?\n",
    "\n",
    "Тяжело. Долго. Больно и мучительно.\n",
    "\n",
    "![alt text](attach3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И где же здесь Deep Learning?\n",
    "\n",
    "Вот он, выделен оранжевым!\n",
    "\n",
    "![alt text](attach2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep NLP\n",
    "\n",
    "***КАК И ВСЕГДА*** — идея в том, чтобы превратить наши входные данные в некоторые непрерывное (=дифференциируемое) предсталвнеие — например, набор точек в N-мерном вектором пространстве (например, N-мерное пространство, где каждое слово имеет 0 во всех позициях, кроме какой-то одной):\n",
    "\n",
    "![alt text](attach4.png)\n",
    "\n",
    "Затем эти вектора мы можем подать на вход нейросети, натренировать на какой-то (далее по тексту) задаче, и на выходе полученные веса и будут вектором, каждое соответствующему какому-то отдельному слову:\n",
    "\n",
    "![alt text](attach5.png)\n",
    "\n",
    "Мы хотим, чтобы семантически \"близкие\" слова находились в этом N-мерном пространстве ближе, а менее — дальше:\n",
    "\n",
    "![alt text](attach6.png)\n",
    "\n",
    "Как это сделать? Правильно — настакать больше слоёв, взять больше данных — то есть делать больше вычислений!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec\n",
    "\n",
    "- Для каждого слова мы попыт аемся натренировать систему, которая предсказывает, что встречается рядом с ним (будем называть близлижайшие слова как \"окно контекста\"):\n",
    "\n",
    "![alt text](attach7.png)\n",
    "\n",
    "Мы хотим натренировать систему, которая для входного слова будет выдавать, какие слова по вероятности чаще всего встречаются рядом с ним — и, если система хорошо это предсказывает, мы можем с наятжкой говорить, что система \"понимает\", о чём идёт речь.\n",
    "\n",
    "![alt text](attach8.png)\n",
    "\n",
    "В идеале, чем меньше входных данных мы обработаем, тем более точно будут определены вероятности, чтоо идёт дальше. Этот подход называется **skip-gram**.\n",
    "\n",
    "Пример (допустим, мы для тех же данных и слова fox хотим сделать максимизацию солва quick):\n",
    "\n",
    "![alt text](attach9.png)\n",
    "\n",
    "**Почему матрицы U и V? Зачем две матрицы?**\n",
    "\n",
    "Это нужно для дополнительной факторизации, чтобы первая матрица, получающая огромный вектор на вход, выдала на выход матрицу размером hidden size, а другая получала на вход значения hidden size — мы заставляем систему каждому слову дать некоторый вектор, таким образом значительно уменьшаем количество параметров (относительно, например, архитектуры с одной матрицей $nхn$).\n",
    "\n",
    "Упрощая, можно думать о ник их как \"матрица центральных/входных слов\" и \"матрица контекстных/выходных слов\" (в некотором смысле это похоже на ранее встречавшуюся систему энкодерая+декодера)... и что в первой, что во второй есть информация о словах — поэтому мы их складываем, чтобы выжать для весов максимум информации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](attach10.png)\n",
    "\n",
    "https://www.youtube.com/live/MBQdMQUZMQM?t=2260\n",
    "\n",
    "И вот, предположим, мы это сделали — мы создали сеть (причём настолько крутую сеть, что в ней даже нет нелинейности), которая так и работает... Проблема в том, что такая система потребует очень большого кол-ва памяти! *Что делать? ©*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sampling\n",
    "\n",
    "С этой проблемой призван бороться **Negative sampling**. Идея в том, чтобы матрица весов V использовалась не вся, а только её части. Нам надо, чтобы на Softmax у нужного нам слова веротянгсть была большая, а у остальных — маленькая.\n",
    "\n",
    "Поэтому, вместо использования всей матрицы, для этого случайно (прям понолстью случайно — ничего страшного, даже если пару раз мы выберем позитивные слова, при достаточно количестве входных данных на loss это практически не повлияет) выберем из матрцы V несколько (например, 5 или 10) слов. \n",
    "\n",
    "После этого, вместо мультиклассовой классификации мы делаем бинарнуюи получаем 1 для нужного слова и 0 для остальных (которые мы случайно выбрали ранее):\n",
    "\n",
    "![alt text](attach11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Свойства word2vec\n",
    "\n",
    "Итак, главное, что мы хотим такой задачей получить в процессе работы — чтобы система натренировала (как части матриц U и V) некоторые вектора, соответствующие некоторым словам, и обладающие семантическим смыслом.\n",
    "\n",
    "Напримеер:\n",
    "\n",
    "$w(Paris) - w(France) + w(Russia) = w(Moscow)$\n",
    "\n",
    "![alt text](attach12.png)\n",
    "\n",
    "Эта система векторов обладает свойством сходства (похожие слова расположены близко друг к другу, и наоборот) — именно то, чего мы добивались!\n",
    "\n",
    "![alt text](attach13.png)\n",
    "\n",
    "Такую систему можно использовать, например, для автоматической категоризации/оценки отзывов на \"позитивные\" и \"негативные\":\n",
    "\n",
    "![alt text](attach14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to The ~~Present~~ всё-таки Past\n",
    "\n",
    "### fastText от Facebook\n",
    "\n",
    "![alt text](attach15.png)\n",
    "\n",
    "- Лучше word2vec\n",
    "- Решает проблему OOV (когда слова нет в словаре)\n",
    "- Есть предтренированные вектора\n",
    "- Работает из коробки\n",
    "- Хорошо работает с иероглифами (внезапно)\n",
    "\n",
    "### BERT, ELMO\n",
    "\n",
    "Первые языковые модельки от OpenAI :3\n",
    "\n",
    "Используют pretrained vectors, чтобы не обучать с нуля, а быстро fine-tunить для нужных задач. Также, благодаря им же, хорошо работают с маленькими объёмами текста.\n",
    "\n",
    "![alt text](attach16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ЧТО ЖЕ СЛУЧИТСЯ В СЛЕДУЮЩЕЙ СЕРИИ???\n",
    "\n",
    "🌌️ <!-- (эр эн эн) -->\n",
    "\n",
    "### УЗНАЕМ В СЛЕДУЮЩЕЙ СЕРИИ!!!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
