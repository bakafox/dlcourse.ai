{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Backpropagation с матрицами. Введение в PyTorch. Инициализация весов. Улучшенные алгоритмы градиентного спуска (Adam, RMSProp, итд).*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На прошлой лекции...\n",
    "- Что такое перцептрон?\n",
    "- Что значит формула $o = f(\\sum(w_i*x_i))$? Что означают её составляющие?\n",
    "- Что такое линейная фукнция (функция активации), зачем она вообще нужна?\n",
    "\n",
    "![alt text](attach1.png)\n",
    "\n",
    "- Что происходит на графе вычислений ниже?\n",
    "\n",
    "![alt text](attach2.png)\n",
    "\n",
    "- Как работает метод оптимизации \"Обратное распространение ошибки\"?\n",
    "\n",
    "![alt text](attach3.png)\n",
    "\n",
    "Если на последний вопрос не удалось ответить, ничего страшного, ведь далее идёт..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation на примере матриц"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данную функцию $f=\\frac12 ||X*W||^2_2$ распишем как граф вычислений:\n",
    "\n",
    "![alt text](attach4.png)\n",
    "\n",
    "Сперва происходит матричное перемножение матрицы входных значений весов на матрицу весов, затем берём L2-норму (https://ru.stackoverflow.com/questions/1124094/l1-%D0%B8-l2-%D1%80%D0%B5%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F-l1-%D0%B8-l2-%D0%BD%D0%BE%D1%80%D0%BC%D0%B0), затем умножаем на $\\frac12$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward (прямой проход)\n",
    "\n",
    "![alt text](attach5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward (обратный проход)\n",
    "\n",
    "Наш верный друг — **дифференциирование сложной функции**: $\\frac{da}{db} = \\frac{da}{db} * \\frac{db}{dc}$\n",
    "\n",
    "![alt text](attach6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Полносвязная сеть, работающая на поочерёдных Forward и Backward — очень эффективная схема для решения задач классификации!\n",
    "\n",
    "![alt text](attach7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры таких задач:\n",
    "\n",
    "![alt text](attach8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "Один из лучших инструментов (библиотек) для написания сетей, типа как выше, на Python.\n",
    "\n",
    "![alt text](attach9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблема Vanishing Gradients\n",
    "\n",
    "При применении гладких функций для функции активации, на выходе у нас большинство входящих значений будут где-то в серединке по распределению, и такая ФА будет их гасить и выдавать примерно ноль.\n",
    "\n",
    "В результате, через какое-то время у части нейронов будет получаться 0 при обратном распространении при дифференциировании, то есть И ИХ И ВСЕ ПРОШЛЫЕ ЗНАЧЕНИЯ ПОТЕРЯЮТСЯ.\n",
    "\n",
    "![alt text](attach13.png)\n",
    "\n",
    "Поэтому ReLU работает лучше — кроме частных случаев вроде случая ниже. С последними призваны бороться его модификации вроде Leaky ReLU или Exponential ReLU:\n",
    "\n",
    "![alt text](attach14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг данных (data preprocessing)\n",
    "\n",
    "Для получения более лучших результатов часто бывает полезно подготовить датасет — выровнять по оси (вычесть среднее знасение из всех значений), нормализировать (масштабировать по стандартному отклонению) и т.д.\n",
    "\n",
    "Эта процедура применяется для всех категорий данных — и для train, и для val, и для test!\n",
    "\n",
    "*На самом деле, сеть статистически научится делать это самостоятельно всё равно, но не сразу, и таким образом мы ускоряем тренировку сети.*\n",
    "\n",
    "![alt text](attach10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обновление весов/параметров\n",
    "\n",
    "![alt text](attach11.gif)\n",
    "\n",
    "(на самом деле линии не будут такими прямыми из-за SGD, но это всего лишь демонстрация)\n",
    "\n",
    "**Классический метод:** `w = w - learning_rate * gradient` (приближаемся с константной скоростью, поэтому подходим всегда небыстро)\n",
    "\n",
    "**Momentum (импульс):** `momentum = от 0.9 до 0.99; velocity = momentum * velocity - learning_rate * gradient; w = w + velocity` (попытка симуляторать физический эффект импульса, например, качения шарика)\n",
    "\n",
    "*Есть и более крутые, но более сложные, способы.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вариации алгоритма градиентого спуска\n",
    "\n",
    "![alt text](attach12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*надеюсь, кто-то живой ещё...* -(c)\n",
    "\n",
    "Да норм, спать охота правда, 3:44 уже, а мне ещё с микриками копаться, потому что контроллер на руках только до сегодня до 9 утра. **Кафеёк! <3**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
