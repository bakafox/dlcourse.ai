{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Линейный классификатор - нейронная сеть с одним слоем. Softmax, функция потерь cross-entropy. Тренировка с помощью стохастического градиентного спуска, регуляризация весов. Многослойные нейронные сети, fully-connected layers. Алгоритм backpropagation.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ЧТО ВАЖНО ЗНАТЬ ДЛЯ ЛУЧШЕГО ПОНИМАНИЯ ЛЕКЦИИ?\n",
    "\n",
    "**Дифференциирование сложной функции:** https://www.youtube.com/watch?v=cFHdrl-E5O4 *(оч важно)*\n",
    "\n",
    "**Градиент:** https://www.youtube.com/watch?v=MKE6jXmvFwg *(ну тоже желательно повторить)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такое нейронная сеть\n",
    "\n",
    "**Нейронная сеть** — это сеть (спасибо К.О), состоящая из математических аналогов биологических нейронов — перцептронов:\n",
    "\n",
    "![alt text](attach1.png)\n",
    "\n",
    "Он принимает некоторые значения (входные данные или выходные значения других нейронов), перемножает их на собственно вычисленные веса (например, в зависимости от важности входных данных для признаков), суммирует их и применяет фукнцию активации (преобразует сумму к дробному значению от $0$ до $1$). Полученное значение и будет выходным значнеим нейрона."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейный классификатор\n",
    "\n",
    "Зная информацию выше, мы можем построить простейшую нейронную сеть — каждый нейрон берёт все данные на вход, перемножает на некоторые (какие? далее по тексту) веса, добавляет некоторые (какие? далее по тексту) параметры и даёт окончательный выход:\n",
    "\n",
    "![alt text](attach2.png)\n",
    "\n",
    "Такая простейшая сеть называется **линейным классификатором.**\n",
    "\n",
    "Его работа сводится к формуле $y = X * W + B$, где\n",
    "\n",
    "- $X$ — набор входных данных\n",
    "- $W$ — набор весов\n",
    "- $B$ — bias; некоторый набор гиперпараметров\n",
    "- $y$ — результаты предсказания\n",
    "\n",
    "Пример линейного классификаторами с вычислениями вручную (перемножение и сложение матриц):\n",
    "\n",
    "![alt text](attach3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Но как мы находим, КАКИЕ веса ($w$) / параметры ($b$) нам нужны???\n",
    "\n",
    "**Методом градиентного спуска!** (чуть дальше он будет объясняться более подробно, но пока только введение)\n",
    "\n",
    "![alt text](attach4.png)\n",
    "\n",
    "Изначально мы берём случайные значения $w$ или $b$, после чего считаем градиент функции потерь и получаем \"направление\", в которое нужно \"двигаться\", чтобы улучшить результат. После этого мы \"двигаемся\" туда на некоторый шаг и повторяем процедуру снова и снова, пока после какого-то числа шагов не придём к некоторому достаточно хорошему, уже мало меняющемуся (**оптимальному**) значению.\n",
    "\n",
    "Это будет означать, что мы пришли к некоторому локальному — в идеальном случае глобальному — минимуму."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "Нам нужно каким-то образом свести результаты предсказания $y$ до какого-то числа, чтобы уже их подставить в метод градиентного спуска. Нам нужно привести их к каким-то вероятностям ($0..1$), сумма которых равна $1$.\n",
    "\n",
    "Воспользуемся для этого формулой мягкого максимума **Softmax**:\n",
    "\n",
    "![alt text](attach5.png)\n",
    "\n",
    "Для результата предсказания $y_n$ (в формуле ниже — $y_0$) формула Softmax будет выглядеть так:\n",
    "\n",
    "![alt text](attach6.png)\n",
    "\n",
    "(экспонента 0-го результата пресказания, делённая на сумму экспонент результатов предсказания)\n",
    "\n",
    "*Таким образом, на этом этапе мы переводим результаты перемножения матриц (предсказаний) в вероятности ($0..1$).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Ахтунг! Дальше идёт вынос моска!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Принцип макисмального правдоподобия\n",
    "\n",
    "![alt text](attach7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя формулу справа сверху (красная $П$ сверху — перемножение, работает аналогично сумме $Σ$), мы можем посчитать **\"вероятность датасета\"** (насколько хорошо подобранные $w$ и $b$ позволяют предсказать значения).\n",
    "\n",
    "Результатом формулы является перемножение для всех классов, что класс ($s$) был угадан верно (т.е. что $gt(s)$ равно $x(s)$).\n",
    "\n",
    "*Формула ниже — просто более оптимизированная для вычислений формула сверху. Её сложнее понять, но делает она, в общем-то, всё то же самое.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регуляризация\n",
    "\n",
    "![alt text](attach8.png)\n",
    "\n",
    "Суть регуляризации в том, чтобы сделать функцию плавнее — таким образом мы усложняем задачу оптимизации, но улучшаем приближение вероятностей на новых (например, тестовых) данных — таким образом боремся с оверфиттингом.\n",
    "\n",
    "На картинке выше, в зелёной рамке — один из множества разных способов регуляризации, **L2-нормализация**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный спуск\n",
    "\n",
    "![alt text](attach9.png)\n",
    "\n",
    "![alt text](attach17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стохастический градиентный спуск (Stochastic Gradient Descent, SGD)\n",
    "\n",
    "*\"стохастический\" здесь означает что-то вроде \"случайный\"*\n",
    "\n",
    "На практике, посчитать градиент для большого числа данных — дело невероятно время- и ресурсозатратное.\n",
    "\n",
    "SGD позволяет очень сильно оптимизировать процесс вычисления (\"схождения\") градиента — вместо всех (здесь) элементов мы берём небольшую, случайного их часть (minibatch, мини-датасет).\n",
    "\n",
    "![alt text](attach10.png)\n",
    "\n",
    "**Что происходит на картинке выше:**\n",
    "1. Из датасета на $50000$ элементов случайно выберем минибатч из $128$ -> получим матрицу $128\\times3072$\n",
    "2. Перемножим их на матрицу весов: $128х3072 * 3072х10$ -> получим матрицу $128х10$\n",
    "3. *(если имеются, иначе сразу на шаг 4)* Добавим наши параметры: $128х10 + 1х10$ -> получим матрицу результатов $128х10$\n",
    "4. Используем Softmax -> получим матрицу результатов от $0$ до $1$ (вероятности) $128х10$\n",
    "5. Вычисляем градиентный спуск, сумму берём не для всего датасета, а только для выбранного минибатча\n",
    "6. Делаем градиентный шаг на веса\n",
    "7. Повторяем с 1-го шага для нового случайного минибатча на новых весах и параметрах; останавливаемся только тогда, когда точность перестаёт уменьшаться (т.е. шаг от шага точность просто \"шатается\" туда-сюда)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Дополнительные ресурсы на тему (на англюсике), если не понятно, что вообще происходит выше:\n",
    "\n",
    "https://cs231n.github.io/linear-classify/\n",
    "\n",
    "https://cs231n.github.io/optimization-1/\n",
    "\n",
    "https://cs231n.github.io/optimization-2/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Так вот, нейронные сети\n",
    "\n",
    "Итак, выше мы отравились мата́ном, а ещё выше — рассмотрели одну из простейших нейронных сетей:\n",
    "\n",
    "![alt text](attach11.png)\n",
    "\n",
    "Мы можем усложнять её конструкцию, просто добавляя больше слоёв весов.\n",
    "\n",
    "![alt text](attach18.png)\n",
    "\n",
    "Но просто перемножение такого вида будет эквивалентно просто перемножению на произведение весов — что же делать? Вставить между шагами некоторую *нелинейную функцию*, или **фукнцию активации**:\n",
    "\n",
    "![alt text](attach12.png)\n",
    "\n",
    "*Примеры фукнций активации (TLDR — раньше почти всегда использовали сигмоиду или tanh, теперь — ReLU, потому что оказалось, что такая куда менее нелинейная фукнция даёт результаты лучше более \"плавных\" (линейных) функций):*\n",
    "\n",
    "![alt text](attach13.png)\n",
    "\n",
    "И именно по_этому принципу работает большинство нейросетей:\n",
    "\n",
    "![alt text](attach19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как считать этотсамый... градиент?\n",
    "\n",
    "Представим формулу вычисления выше как направленный **граф вычислений** (**computational graph**):\n",
    "\n",
    "![alt text](attach14.png)\n",
    "\n",
    "Как мы можем выполнить эти вычисления эффективно? С помощью алгоритма **обратного распространения ошибки (Back Propagation)!**\n",
    "\n",
    "Как он работает? Рассмотрим на \"игрушечном\" примере для некоторой функции $f$:\n",
    "\n",
    "![alt text](attach20.png)\n",
    "\n",
    "Мы можем посчитать градиенты аналитически, но мы не хотим этого делать.\n",
    "\n",
    "Вместо этого мы можем воспользоваться формулой производной сложной функции (обведена зелёным), чтобы посчитать градиент С КОНЦА!\n",
    "\n",
    "![alt text](attach15.png)\n",
    "\n",
    "Обратным проходом идя по графу и используя предыдущие вычисления, только локальными вычислениями (т.е. не выписывая формулу $f$ целиком) по одному разу, вычислить производные и градиенты для каждого узла графа. В случае с нейросетями, вместо чисел будут матрицы, но принцип останется прежним. \n",
    "\n",
    "В коде на Python это выглядит так (forward — прямой проход, backward — обратный проход):\n",
    "\n",
    "![alt text](attach16.png)\n",
    "\n",
    "Таким образом, это даёт нам возможность пройтись (например, в графе вычислений выше) от расчитанной фукнции потерь обратно до начальных значений, скорректировав их, и затем повторять циклично этот процесс прохода вперёд-назад."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Всё ещё ничего не понятно!!!1\n",
    "\n",
    "#### 3Blue1Brown: https://www.youtube.com/watch?v=Ilg3gGewQ5U\n",
    "\n",
    "*Фухххххххххххх, картинка на превьюшке видео прям идеально описывает как себя чувствуешь после этой лекции. Хотя, с его слов, дальше будет проще, если разобраться, что происходит здесь... верим, я поверил (спойлер — так оно и есть!).*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
