{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*GPUs. Процесс тренировки и overfitting/underfitting на практике,. Learning rate annealing. Batch Normalization. Ансамбли. Что нового в 2018.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(последняя из лекций про нейросети в целом; далее речь будет идти только о конкретных областях и реализациях применения)\n",
    "\n",
    "Если что-то непонятно (здесь или далее), ОЧЕНЬ ВАЖНО **вернуться к (по факту, самым необходимым для понимания) лекциям 1–4 и разобраться**!!! Да-да!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Почему обучение на видеокартах (GPU)?\n",
    "\n",
    "*(тут мне и так всё понятно, расписывать ничего не хочу и не буду)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Secret Ingredient is... Training Process\n",
    "\n",
    "![alt text](attach1.png)\n",
    "\n",
    "Ключом к успешному обучение нейросети является чёткое понимание процесса тренировки — куда всё движется и что делать, чтобы перейти от underfit/overfit к optimal.\n",
    "\n",
    "*Для Torch есть полезный инстумент для анализа метрик **Tensorboard**.*\n",
    "\n",
    "Потренируемся по графикам (некоторой метрики точности у val и train) определять, в каком \"режиме\" находится модель, и что с этим делать:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ситуация №1\n",
    "\n",
    "Train loss и Val loss уменьшаются медленно и идут близко друг к другу — это overfit, underfit или всё OK? **ОТВЕТ ПОД КАРТИНКОЙ**\n",
    "\n",
    "![alt text](attach2.png)\n",
    "\n",
    "Это **underfitting** — модель недостаточно сложная или процесс тренировки слишком слабый.\n",
    "\n",
    "#### Что можно сделать?\n",
    "\n",
    "- Из прошлых лекций:\n",
    "    - Взять модель помощнее\n",
    "    - Изменить подход к данным\n",
    "    - Использовать Preprocessing, если он ещё не используется\n",
    "    - Использовать хороший метод тренировки (например, Adam вместо обычного градиентного спуска) \n",
    "- Из нового:\n",
    "    - (довольно очевидное) Попробовать подобрать (экспериментальным путём — а как иначе?) Learning Rate\n",
    "    - (менее очевидное) **Learning Rate Annealing (Decay)**:\n",
    "\n",
    "        ![alt text](attach3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ситуация №2\n",
    "\n",
    "Модель тренируется довольно хорошо — train продолжает падать и падать... но val через какое-то время перестаёт уменьшаться, и даже начинает увеличиваться! Что это такое? **ОТВЕТ ПОД КАРТИНКОЙ**\n",
    "\n",
    "![alt text](attach4.png)\n",
    "\n",
    "Это типичный случай **overfitting** — модель, по сути, тупо запомнила тренировочные данные, а не их конкретные признаки, и начала терять способность распознавать реальные данные.\n",
    "\n",
    "#### Что можно сделать?\n",
    "\n",
    "- Из прошлых лекций:\n",
    "    - **Регулязиарция** (например, уже известная нам **L2-регуляризация**, или **Dropout** (обычно 1–2 раза в конце сети)):\n",
    "\n",
    "        ![alt text](attach5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "#### Есть ли универсальные методы И для overfit, И для underfit?\n",
    "\n",
    " Да, и BN — самый известный из такиз методов. Он работает с проблемой \"взрывающихся (улетающих высоко вверх) и затухающих (улетающих в ноль) нейронов\", добиваясь ситуации, чтобы \"сила\" активаций нейронов была примерно одного масштаба.\n",
    "\n",
    "![alt text](attach6.png)\n",
    "\n",
    "Принцип работы — batchnorm линейно масштабирует (умножит на что-нибудь и что-нибудь прибавит) выход слоя после взвешивания так, чтобы у весов было предсказуемое среднее и стандартное отклоненени.\n",
    "\n",
    "![alt text](attach7.png)\n",
    "\n",
    "![alt text](attach8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск гиперпараметров\n",
    "\n",
    "#### Что мы имеем в виде под гиперпараметрами?\n",
    "\n",
    "- Начальный Learning Rate\n",
    "- Коэффициент Learning Rate Annealing\n",
    "- Коэффициент L2-регуляризации\n",
    "- Размер batch\n",
    "- ...\n",
    "\n",
    "#### TLDR — методом тыка — сначала просто наугад, потом \"по чуйке\".\n",
    "\n",
    "Общие рекомендации для этого процесса:\n",
    "1. Сперва ищем в широком диапозоне, пока не находим +- оптимальный вариант. А после этого начинаем немного от него отходить в одну-другую сторону, где лучше результат.\n",
    "2. Log space:\n",
    "    - `lr = random(10, 10000)` — ПЛОХО\n",
    "    - `lr = 10**random(1, 4)` — ХОРОШО"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Короче! Начинать с...**\n",
    "1. **Preprocessing** — Вычитаем среднее из данных\n",
    "2. Оптимизатор — **Adam**\n",
    "3. **Batch Normalization** — добро\n",
    "4. **Learning Rate Annealing на плато**\n",
    "5. Перебираем гиперпараметры; важнее всего **learning rate**\n",
    "6. **СМОТРИМ НА ГРАФИКИ ТРЕНИРОВКИ!**\n",
    "\n",
    "---\n",
    "\n",
    "А для получения дополнительных % точностей уже ближе к концу процесса создания модели можно использовать вот эти трюки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ансамбль моделей\n",
    "\n",
    "*(к музыкальным ансамблям отношения практически никакого не имеет, just in case)*\n",
    "\n",
    "![alt text](attach9.png)\n",
    "\n",
    "Если натренировать не одну модель, а несколько, желательно поразнообразнее, но каждая имеет хорошую точность, а затем их предсказания усреднить, то итоговая точность будет выше на 1–5%!\n",
    "\n",
    "#### Откуда брать разнообразные модели?\n",
    "- **Использовать разные модели и подходы**\n",
    "- **Cross-validation folds** (усреднение одной и той же модели, натренированной на разных trans/val/test-разбиениях датасета)\n",
    "- ***Креатив!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BACK TO THE \"PRESENT\"\n",
    "\n",
    "*Свежие (ну, почти) подходы из 2018–2019 гг.*\n",
    "\n",
    "#### LR Range Test + Cyclical Learning Rate\n",
    "\n",
    "Отдельный экспериментаьный запуск на train исключительно для подбора оптимального диапазона LR — по ходу обучения через $N$ эпох Learning Rate постепенно повышается от околонулевого до очень большого, а затем анализируем график точности.\n",
    "\n",
    "Оттуда мы извлкаем минимальный и максимальный Learning Rate.\n",
    "\n",
    "Затем, при уже нормальной тренировке модели, мы циклически повышаем-понижаем LR от минимально-оптиматльного до макисмально-оптимального, в надежде, что это поможет нейронке \"выбраться\" из локального минимума и найти точку более низкую, тем самым получить точность предсказания повыше.\n",
    "\n",
    "На практике, это почти всегда помогает прийти либо к более, либо хотя бы к не менее точной точности модели.\n",
    "\n",
    "![alt text](attach10.png)\n",
    "\n",
    "#### Инженерный подход к перебору гиперпараметров\n",
    "\n",
    "![alt text](attach11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (для общего развития) Что и как используют в проде?\n",
    "\n",
    "![alt text](attach12.png)\n",
    "\n",
    "На этом всё! Дальше лекции будут более конкретными по отедльным темам."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
