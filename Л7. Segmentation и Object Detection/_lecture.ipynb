{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Более сложные задачи компьютерного зрения - сегментация (segmentation) и нахождение объектов на изображении (object detection).*\n",
    "\n",
    "*Читает Владимир Игловиков*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(лол, в 2к19 челибобер сидит на Unity)*\n",
    "\n",
    "# Повторим, шо такое клаффисикация\n",
    "\n",
    "Берётся какая-то картинка, потом конволюционный слой, потом активация, может быть BatchNorm, потом pulling-слои... в конце концов, pulling-слой, Fully Connected Layer и предсказание:\n",
    "\n",
    "![alt text](attach1.png)\n",
    "\n",
    "То есть, много слоёв, много параметров...\n",
    "\n",
    "Но хотелось бы автjматически решать и другие задачи — например, извлекать информацию о цветах или точно занимаемых положениях объекта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Segmentation\n",
    "\n",
    "![alt text](attach2.png)\n",
    "\n",
    "1. **Сегментация = попиксельная классификация**; про каждый пиксель можно, для упрощения, думать как про попиксельный классификатор\n",
    "2. **Не требует большого объёма тренировочных данных**; но сами данные, при этом, готовить гораздо тяжелее\n",
    "3. **Все сегментационные сети — это архитектуры вида FCN**\n",
    "4. На выход, как правило, выдаёт картинку такого же разрешения, что и на вход\n",
    "\n",
    "### Метрики\n",
    "\n",
    "![alt text](attach3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Convolutional Network (FCN)\n",
    "\n",
    "![alt text](attach4.png)\n",
    "\n",
    "\\***Dense-слои** — это другое название FC- aka полносвязных слоёв)\n",
    "\n",
    "### FCN = Efficient Sliding Window\n",
    "\n",
    "Как можно пытаться делать предсказание? Скользящим окном (взять какой-то кроп из куска картинки и пытаться предсказывать центральный пиксель; см. прошлые лекции). Но это очень неэффективно!\n",
    "\n",
    "Наша полносвязная сеть выше позволет делать то же, что и скользящее окно, но гораздо более эффективно:\n",
    "\n",
    "![alt text](attach5.png)\n",
    "\n",
    "(картинка иллюстрирует, что если мы возьмём input чуть побольше, то мы просто получим output чуть побольше — но оно сработает)\n",
    "\n",
    "Идея в том, что в силу работы свёрточных слоёв мы можем применять FCN к input-ам **любого размера** — и это соответствует тому, что если бы мы применяли свёрточную сеть со скользящим окном (из-за того, какая математика в convolution слоях). Это позволяет нам проанализировать картнку гораздо быстрее, чем если бы мы прогоняли RCC сеть много раз по разным разрешениям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Classification to Segmentation\n",
    "\n",
    "На картинке ниже — FCN8, одна из первых попыток реализовать сегментационную нейросеть на основе классификационной:\n",
    "\n",
    "![alt text](attach6.png)\n",
    "\n",
    "То есть, мы, \"оторвав\" полносвязный слой, получаем heatmap, а потом делаем upsampling до исходного размера.\n",
    "\n",
    "\\***upsampling** — повышаем разрешение, обычно применяя некоторую интерполяцию (чтобы получить менее \"грубое\" увеличение), как правило билинейную или бикубическую)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. FCN8 to SegNet\n",
    "\n",
    "*А давайте, вместо того, чтобы делать upsampling сразу намного, будем делать его постепенно и по чуть-чуть, перемежая его с другими слоями?* \n",
    "\n",
    "(upsampling -> refinement (conv + BN + ReLU) -> повторяем)\n",
    "\n",
    "![alt text](attach7.png)\n",
    "\n",
    "Это позволило сделать границы получамого ихображения более плавными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SegNet to UNet\n",
    "\n",
    "*Давайте добавим Skip Connection-ы!*\n",
    "\n",
    "![alt text](attach8.png)\n",
    "\n",
    "\\***Skip Conenction** — переносим оригинальные данные с какого-то слоя в какой-то более поздний слой без изменений, обычно в качестве части данных, к которым конкатинируются/добавляется выход со слоя/слоёв.\n",
    "\n",
    "Примерно то же самое, что Residiual-слои в ResNet, но здесь используются не для улучшения протекаемости градиентов, а для улучшения точности границ.\n",
    "\n",
    "![alt text](attach10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. U-Net to TernausNet\n",
    "\n",
    "*Давайте, вместо тренировки сети с нуля, загружать уже предтренированные веса!*\n",
    "\n",
    "![alt text](attach9.png)\n",
    "\n",
    "(TernausNet - просто пример, по такому принципу работает множество разных моделей на основе архитектиуры U-Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. U-Net + FPN\n",
    "\n",
    "![alt text](attach12.png)\n",
    "\n",
    "![alt text](attach11.png)\n",
    "\n",
    "<!-- Снимаем предсказания со всех слоёв вместо последнего, то есть работаем с heatmap-ами масками разного разрешения, получаем сегментационные маски разного размера, upsampl-им их до одного размера, конкатериуем и делаем предсказание. -->\n",
    "\n",
    "С каждого слоя (а не только последнего) decoder-а (т.е. когда мы \"разжимаем\" картинку) снимаем feature map-ы (разных масштабов), upscal-им их до одного (самого большого) размера, конкатенируем и делаем предсказание.\n",
    "\n",
    "А так как последние слои тоже тренируемые, модель \"учится\" учитывать признаки как маленьких, так и больших объектов — таким образом FPN помогает для определения и маленьких и больших объектов на картинках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "I HATE ADHD I HATE ADHD I HATE ADHD I\n",
    "HATE ADHD I HATE ADHD I HATE ADHD I HATE\n",
    "ADHD I HATE ADHD I HATE ADHD I HATE ADHD\n",
    "I HATE ADHD I HATE ADHD I HATE ADHD I\n",
    "HATE ADHD I HATE ADHD I HATE ADHD I HATE\n",
    "ADHD I HATE ADHD I HATE ADHD I HATE ADHD\n",
    "I HATE ADHD I HATE ADHD I HATE ADHD I\n",
    "HATE ADHD I HATE ADHD I HATE ADHD I HATE\n",
    "ADHD I HATE ADHD I HATE ADHD I HATE ADHD\n",
    "I HATE ADHD I HATE ADHD I HATE ADHD I\n",
    "HATE ADHD I HATE ADHD I HATE ADHD I HATE\n",
    "ADHD I HATE ADHD I HATE ADHD I HATE ADHD\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](attach13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Detection\n",
    "\n",
    "### Detection Metric: mAP\n",
    "\n",
    "Median Anerage Precision\n",
    "\n",
    "> *самая замороченная метрика ©*\n",
    "\n",
    "![alt text](attach14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Maximum Supression (NMS)\n",
    "\n",
    "![alt text](attach15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Виды детекции (Detecion)\n",
    "\n",
    "![alt text](attach16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### One-shot Detectors\n",
    "\n",
    "— выполняют классификацию, отрывают последний слой (т.е. получают heatmap-у), затем разбивают входные данные на сетку.\n",
    "\n",
    "![alt text](attach17.png)\n",
    "\n",
    "Далее берут исходную картинку, делят на регионы, и предсказывают сразу 5 значений — `[precision, x, y, w, h]` (или типа того) — в каждом регионе. Так делаеют для всех предсказаний из классификатора, прошедших некоторый threshold.\n",
    "\n",
    "Оин из наиболее популярных One-shot детекторов — **YOLO** (You Only Look Once):\n",
    "\n",
    "![alt text](attach18.png)\n",
    "\n",
    "**SSD** (нет, не тверодительый накопитель) — модификация YOLO с multi-scale детекцией. Это заметно улучшает точность (см. метрику mAP (Median Average Precision)), но немного понижает скорость работы (см. метрику FPS (Frames Per Second)):\n",
    "\n",
    "![alt text](attach19.png)\n",
    "\n",
    "*Позже эта же архитектура появится в 3-й версии YOLO, **YOLOv3**, и будет использоваться далее и далее.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Two-shot Detectors\n",
    "\n",
    "— сперва \"находят\" **proporsals** (предполагаемые регионы объектов), затем каждый proporsal прогоняют через классификационную сеть.\n",
    "\n",
    "![alt text](attach20.png)\n",
    "\n",
    "![alt text](attach21.png)\n",
    "\n",
    "![alt text](attach22.png)\n",
    "\n",
    "Performance:\n",
    "\n",
    "![alt text](attach23.png)\n",
    "\n",
    "Improvements:\n",
    "\n",
    "![alt text](attach24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как работать с этим всем на практике?\n",
    "\n",
    "### Для задач сегментации:\n",
    "\n",
    "1. Можно начать с экспериментов U-Net (или TernausNet, по сути тот же U-Net, но с возможностью брать предтренированные веса) — во всяком случае, она подойдёт для большинства задач\n",
    "\n",
    "2. Иногда, для более солжных задач, можно взять U-Net + FPN\n",
    "\n",
    "### Для задач детекции:\n",
    "\n",
    "1. Для one-shot детекции — конечно же, YOLO (YOLOv5 или YOLOv8/YOLOv9)!\n",
    "2. Для two-shot детекции — см. ссылку на аттаче выше, там докумнетация из Facebook Research, объясняющая выбор и работу разных видов и модификаций RCNN-ок\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's it! ~~You're well on your way to become an osu! Rhythm Champion!~~"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
