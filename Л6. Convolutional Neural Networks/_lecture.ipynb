{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Convolution и pooling layers. Эволюция архитектур: LeNet, AlexNet, VGG, ResNet. Transfer learning. Аугментации.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сперва вспомним, как работают уже известные нам FC-сети:\n",
    "\n",
    "![alt text](<attach1.png>)\n",
    "\n",
    "(вспоминаем ёпта!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В чём проблема таких сетей?\n",
    "\n",
    "В невероятно большом количестве весов и параметров, которые нужно обучить — например, для одной этой картинки кошечки нужно аж $200*200*3*1000$ весов ($W*H*CH*W$) на каждый слой!\n",
    "\n",
    "![alt text](<attach2.png>)\n",
    "\n",
    "Преимущества CNN заключаются в том, что засчёт их архитектуры (далее по тексту) весов и параметров итого получается гораздо меньше, при этом сеть становится более адаптивна к сдвигам по вертикали/горизонтали (но не поворотам) искомого обхекта на картинке:\n",
    "\n",
    "![alt text](attach17.png)\n",
    "\n",
    "*Что здесь вообще происходит? Далее по тексту...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Layer\n",
    "\n",
    "Значение выхода слоя зависит не от всего набора данных, а только от некоторой окрестности — при этом веса для них **остаются одни и те же**:\n",
    "\n",
    "![alt text](<attach3.png>)\n",
    "\n",
    "Разберём на примере (у входного слоя размером $3х3$ 1 канал, у выходного размером $2х2$ — 2):\n",
    "\n",
    "![alt text](<attach4.png>)\n",
    "\n",
    "Получается, мы вот таким квадратиком $2х2$ (\"скользящим окном\") идём по входной матрице $3х3$, перемножаем матрицы $2х2$ (данные в \"окне\" и веса соотв-но) и складываем всё вместе:\n",
    "\n",
    "![alt text](<attach5.png>)\n",
    "\n",
    "Так \"проходимся\" (слева направа, сверху вниз) по всей картинке и для всех каналов (для каждого канала мы используем соответствующие веса, то есть тут для первого канала мы драли $w_{c`=1}$, а для второго мы теперь берём $w_{c`=2}$):\n",
    "\n",
    "![alt text](<attach6.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Почему СВЁРТОЧНЫЕ?\n",
    "\n",
    "Потому что такой проход одной матрицей (\"паттерном\" или \"фильтром\") по другой **эквивалентен операции свёртки** (которая, например, также используется для обработки изображений):\n",
    "\n",
    "![alt text](<attach7.png>)\n",
    "\n",
    "И, по такому принципу, модель может научиться (все веса, как обычно, изначально берутся случайные) использовать разные фильтры и разные области для более эффективной детекции разных признаков!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Закрепим понимание на примере двухканальной картинки\n",
    "\n",
    "![alt text](<attach8.png>)\n",
    "\n",
    "### Как с такой вообще работать?\n",
    "\n",
    "Мы можем разложить наш тензор входных данных $2х2х2$ в вектор $1х(2*2*2)$, а тензор весов — в матрицу $(2*2*2)х1$.\n",
    "\n",
    "После этого мы с лёгкостью можем перемножить матрицы и получить на выход $1х2$:\n",
    "\n",
    "![alt text](<attach9.png>)\n",
    "\n",
    "(то же самое для след. пикселя):\n",
    "\n",
    "![alt text](<attach10.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# А как считать градиент?\n",
    "\n",
    "Интерпретация операций свёртки как перемножения матриц даёт нам понять, что эта операция дифференциируемая — то есть, мы можем найти градиенты весов и входных данных, выполним перемножения на транспонированные матрицы (и входы и вкса) и аккумулировав их:\n",
    "\n",
    "![alt text](<attach11.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция активации\n",
    "\n",
    "![alt text](<attach12.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Несколько слоёв\n",
    "\n",
    "![alt text](<attach13.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дополнение (padding) и шаг (stride)\n",
    "\n",
    "![alt text](attach14.png)\n",
    "\n",
    "(2) **padding** — насколько мы можем выйти за пределы картинки (пиксели за этими пределами заполняются, например, нулями или отражёнными частями картинки); обычно используется, чтобы размер выхода слоя сохранялся такой же, какой был размер входа.\n",
    "\n",
    "(3) **stride** — на сколько пикселей сдвигаемся для применения фильтра; используется для уменьшения вычислений.\n",
    "\n",
    "(4) Мы можем комбинировать stride и padding, как нам удобнее!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling Layer (maxpool, avgpool, minpool)\n",
    "\n",
    "Очень простой слой — выюирает из выходных данных каждое $n$-e (как правило, 2-е) значение (то есть картинка остаётся с таким же кол-вом каналов, но уменьшается в $n$ раз) по какому-то очень простому принципу — например, выбирает из каждой подобласти $nхn$ самые большие значения, либо усредняет их и берёт среднее...\n",
    "\n",
    "![alt text](attach15.png)\n",
    "\n",
    "### Зачем мы это делаем?\n",
    "\n",
    "Таким образом мы \"сжимаем\" данные после прохождения после фильтра, чтобы они содержали меньше бесполезной информации и нейронам требовалось меньше вычислений.\n",
    "\n",
    "*Таким образом, обычно архитектура свёрточной нейронной сети (CNN) выглядит примерно так:*\n",
    "\n",
    "![alt text](attach16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# КОНКРЕТНЫЕ АРХИТЕКТУРЫ, ЮЗАЮЩИЕ CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet '98\n",
    "\n",
    "![alt text](attach18.png)\n",
    "\n",
    "- Создана Ян ЛеКуном в 1998 году для распознавания рукописных чисел в почтовых индексах\n",
    "\n",
    "### IMAGENET\n",
    "\n",
    "![alt text](attach19.png)\n",
    "\n",
    "- Огромный датасет на 1 млн картинок и 1000 классов, позволяющий обучить как очень высококуровневую (кошекчки-собычки), так и очень низкоуровневую (породы собак)\n",
    "- Считается основоположником \"бума\" Deep Learning после 2012 года\n",
    "\n",
    "### AlexNet '12\n",
    "\n",
    "![alt text](attach20.png)\n",
    "\n",
    "- Одна из первых нейронок, обучавшихся при помощи GPU\n",
    "\n",
    "### VGG '14\n",
    "\n",
    "![alt text](attach21.png)\n",
    "\n",
    "### ResNet '15\n",
    "\n",
    "![alt text](attach24.png)\n",
    "\n",
    "- Решила проблему глубины сетей (чем больше слоёв, тем хуже тренируется сеть — в итоге сети 100+ слоёв требуют невероятно много времени) с помощью **Residial Connections**\n",
    "- Проблема был в том, что какой-то далёкий, ещё не обученный слой может \"убить\" всё обучение, поэтому много слоёв делать было нельзя, иначе обучение шло невероятно медленно:\n",
    "\n",
    "    ![alt text](attach22.png)\n",
    "\n",
    "- Идея **Residial Connections** в том, чтобы вместо полного пропускания входных данных мы сразцу отдаёт входные данные плюс какую-то \"поправку\" слоя, которая с ними суммируется. Например, в ResNet они идут через каждые 2 слоя:\n",
    "\n",
    "    ![alt text](attach23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "*Как обучить сеть очень хорошо что-то распознавать на очень малом (<200 картинок) объёме данных*\n",
    "\n",
    "1. Берём хорошо работающую, уже хорошо обученную на чём-то сеть (например, предобученный MobileNet);\n",
    "2. \"**Замораживаем**\" (запрещаем изменение) всех слоёв, кроме нескольких последних;\n",
    "3. Таким образом, тренируем на наших данных только последние несколько слоёв. И это работает, и очень хорошо!\n",
    "\n",
    "![alt text](attach25.png)\n",
    "\n",
    "Такой приём очень очень часто используют на практике, поскольку он позволяет вместо долгого обучения просто зафайнтюнить уже готовую модель (например, ResNet 34 или ResNet 101) под свои нужды.\n",
    "\n",
    "**Более продвинутая реализация TL, для большего числа данных (1000+ картинок):**\n",
    "\n",
    "![alt text](attach26.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Аугментация данных\n",
    "\n",
    "*Где данных взять? Давно известно!*\n",
    "\n",
    "![alt text](attach27.png)\n",
    "\n",
    "Способы применения аугментации:\n",
    "- **Для train — Online Augmentation:**\n",
    "\n",
    "    ![alt text](attach28.png)\n",
    "\n",
    "- **Для val/test — Time Test Augmentation:**\n",
    "\n",
    "    ![alt text](attach29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*omg das literlly me!!!1*\n",
    "\n",
    "![alt text](attach30.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
